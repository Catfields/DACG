#!/usr/bin/env python3
"""
å¿«é€Ÿä¼˜åŒ–è®­ç»ƒé›†è§„æ¨¡ - å¸¦è¿›åº¦æ¡ç‰ˆæœ¬
"""

import json
import random
import time
from collections import defaultdict
from tqdm import tqdm

def create_optimized_dataset():
    """åˆ›å»ºä¼˜åŒ–çš„10ä¸‡æ ·æœ¬æ•°æ®é›†ï¼Œå¸¦è¿›åº¦æ¡æ˜¾ç¤º"""
    
    print("ðŸš€ å¼€å§‹ä¼˜åŒ–è®­ç»ƒé›†...")
    
    # åŠ è½½æ•°æ®
    print("ðŸ“Š åŠ è½½åŽŸå§‹æ•°æ®é›†...")
    with open('data/mimic_cxr/annotation.json', 'r') as f:
        data = json.load(f)
    
    examples = data['train']
    total_samples = len(examples)
    print(f"ðŸ“ˆ æ€»æ ·æœ¬æ•°: {total_samples:,}")
    
    # æŒ‰å—è¯•è€…åˆ†ç»„ - å¸¦è¿›åº¦æ¡
    print("ðŸ” æŒ‰å—è¯•è€…åˆ†ç»„...")
    subject_groups = defaultdict(list)
    for example in tqdm(examples, desc="å¤„ç†æ ·æœ¬", unit="æ ·æœ¬"):
        subject_groups[example['subject_id']].append(example)
    
    print(f"ðŸ‘¥ å”¯ä¸€å—è¯•è€…: {len(subject_groups):,}")
    
    # åˆ†å±‚é‡‡æ · - æ›´é«˜æ•ˆçš„å®žçŽ°
    target_size = 100000
    random.seed(42)
    
    print("ðŸŽ¯ å¼€å§‹åˆ†å±‚é‡‡æ ·...")
    
    # è®¡ç®—æ¯ä¸ªå—è¯•è€…åº”ä¿ç•™çš„æ ·æœ¬æ•°
    sampling_ratio = target_size / total_samples
    
    # é¢„åˆ†é…ç»“æžœåˆ—è¡¨å¤§å°
    selected_examples = []
    
    # ä½¿ç”¨æ›´é«˜æ•ˆçš„é‡‡æ ·ç­–ç•¥
    subject_list = list(subject_groups.items())
    random.shuffle(subject_list)  # éšæœºæ‰“ä¹±å—è¯•è€…é¡ºåº
    
    with tqdm(total=target_size, desc="é‡‡æ ·è¿›åº¦", unit="æ ·æœ¬") as pbar:
        for subject_id, subject_examples in subject_list:
            if len(selected_examples) >= target_size:
                break
                
            keep_count = max(1, int(len(subject_examples) * sampling_ratio))
            
            if len(subject_examples) <= keep_count:
                selected_examples.extend(subject_examples)
                pbar.update(len(subject_examples))
            else:
                selected = random.sample(subject_examples, keep_count)
                selected_examples.extend(selected)
                pbar.update(keep_count)
    
    # ç²¾ç¡®è°ƒæ•´åˆ°10ä¸‡æ ·æœ¬
    final_selected = selected_examples[:target_size]
    print(f"âœ… æœ€ç»ˆé€‰æ‹©æ ·æœ¬æ•°: {len(final_selected):,}")
    
    # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
    unique_subjects = len(set(ex['subject_id'] for ex in final_selected))
    unique_studies = len(set(ex['study_id'] for ex in final_selected))
    avg_report_length = sum(len(ex['report'].split()) for ex in final_selected) / len(final_selected)
    
    print(f"\nðŸ“‹ ä¼˜åŒ–æ•°æ®é›†ç»Ÿè®¡:")
    print(f"   æ ·æœ¬æ•°: {len(final_selected):,}")
    print(f"   å—è¯•è€…: {unique_subjects:,}")
    print(f"   ç ”ç©¶æ•°: {unique_studies:,}")
    print(f"   å¹³å‡æŠ¥å‘Šé•¿åº¦: {avg_report_length:.1f} è¯")
    
    # åˆ›å»ºä¼˜åŒ–æ•°æ®é›†
    print("ðŸ’¾ ä¿å­˜ä¼˜åŒ–æ•°æ®é›†...")
    optimized_data = {
        'train': final_selected,
        'val': data.get('val', []),
        'test': data.get('test', [])
    }
    
    with open('data/mimic_cxr/annotation_100k.json', 'w') as f:
        json.dump(optimized_data, f, indent=2)
    
    print("âœ¨ ä¼˜åŒ–å®Œæˆï¼")
    
    return len(final_selected)

if __name__ == "__main__":
    start_time = time.time()
    
    try:
        final_count = create_optimized_dataset()
        elapsed_time = time.time() - start_time
        print(f"\nðŸŽ‰ å¤„ç†å®Œæˆï¼è€—æ—¶: {elapsed_time:.2f} ç§’")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")