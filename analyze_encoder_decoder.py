#!/usr/bin/env python3
"""
åˆ†æEncoderDecoderä¸­å„ç»„ä»¶çš„å‚æ•°é‡åˆ†å¸ƒ
"""

import torch
import torch.nn as nn
from modules.dacg import EncoderDecoder
from modules.tokenizers import Tokenizer
import argparse

def count_parameters(model, name=""):
    """è®¡ç®—æ¨¡å‹å‚æ•°"""
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    if name:
        print(f"{name}:")
    print(f"  æ€»å‚æ•°é‡: {total_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    print(f"  å†»ç»“å‚æ•°é‡: {total_params - trainable_params:,}")
    return total_params, trainable_params

def analyze_encoder_decoder_components(args):
    """åˆ†æEncoderDecoderå„ç»„ä»¶çš„å‚æ•°é‡"""
    
    # åˆ›å»ºtokenizer
    tokenizer = Tokenizer(args)
    
    # åˆ›å»ºå®Œæ•´æ¨¡å‹
    model = EncoderDecoder(args, tokenizer)
    
    print("=" * 60)
    print("EncoderDecoder ç»„ä»¶å‚æ•°é‡åˆ†æ")
    print("=" * 60)
    
    # åˆ†æå®Œæ•´æ¨¡å‹
    total_params, trainable_params = count_parameters(model, "å®Œæ•´ EncoderDecoder æ¨¡å‹")
    
    print("\n" + "=" * 60)
    print("å„ç»„ä»¶è¯¦ç»†åˆ†æ")
    print("=" * 60)
    
    # åˆ†æè§†è§‰æå–å™¨
    if hasattr(model, 'visual_extractor'):
        visual_params, visual_trainable = count_parameters(model.visual_extractor, "è§†è§‰æå–å™¨ (Visual Extractor)")
    else:
        print("è§†è§‰æå–å™¨: æœªæ‰¾åˆ°ç‹¬ç«‹æ¨¡å—")
        visual_params = 0
        visual_trainable = 0
    
    # åˆ†æç¼–ç å™¨
    encoder_params, encoder_trainable = count_parameters(model.encoder, "ç¼–ç å™¨ (Encoder)")
    
    # åˆ†æè§£ç å™¨
    decoder_params, decoder_trainable = count_parameters(model.decoder, "è§£ç å™¨ (Decoder)")
    
    # åˆ†æGMG (å¼•å¯¼è®°å¿†ç”Ÿæˆå™¨)
    if hasattr(model, 'gmg'):
        gmg_params, gmg_trainable = count_parameters(model.gmg, "å¼•å¯¼è®°å¿†ç”Ÿæˆå™¨ (GMG)")
    else:
        print("å¼•å¯¼è®°å¿†ç”Ÿæˆå™¨: æœªæ‰¾åˆ°ç‹¬ç«‹æ¨¡å—")
        gmg_params = 0
        gmg_trainable = 0
    
    # è¯¦ç»†åˆ†æç¼–ç å™¨å†…éƒ¨
    print("\n" + "=" * 60)
    print("ç¼–ç å™¨å†…éƒ¨ç»„ä»¶åˆ†æ")
    print("=" * 60)
    
    # ç¼–ç å™¨çš„ä¸»è¦ç»„ä»¶
    encoder_components = {
        'dual_attention': model.encoder.dual_attention,
        'input_projection': model.encoder.input_projection,
        'encoder_layers': model.encoder.layers
    }
    
    for name, component in encoder_components.items():
        if hasattr(component, '__len__'):  # æ˜¯ModuleList
            layer_params = sum(p.numel() for layer in component for p in layer.parameters())
            layer_trainable = sum(p.numel() for layer in component for p in layer.parameters() if p.requires_grad)
            print(f"{name}: {layer_params:,} å‚æ•°")
        else:
            params, trainable = count_parameters(component, f"  {name}")
    
    # è¯¦ç»†åˆ†æè§£ç å™¨å†…éƒ¨
    print("\n" + "=" * 60)
    print("è§£ç å™¨å†…éƒ¨ç»„ä»¶åˆ†æ")
    print("=" * 60)
    
    decoder_components = {
        'embedding': model.decoder.embedding,
        'pos_encoding': model.decoder.pos_encoding,
        'decoder_layers': model.decoder.layers
    }
    
    for name, component in decoder_components.items():
        if name == 'decoder_layers':
            # åˆ†ææ¯ä¸€å±‚
            for i, layer in enumerate(component):
                layer_params, layer_trainable = count_parameters(layer, f"  è§£ç å™¨å±‚ {i+1}")
                
                # åˆ†æè§£ç å™¨å±‚çš„å­ç»„ä»¶
                if hasattr(layer, 'masked_mha'):
                    count_parameters(layer.masked_mha, f"    æ©ç å¤šå¤´æ³¨æ„åŠ›")
                if hasattr(layer, 'cross_mha'):
                    count_parameters(layer.cross_mha, f"    äº¤å‰å¤šå¤´æ³¨æ„åŠ›")
                if hasattr(layer, 'feed_forward'):
                    count_parameters(layer.feed_forward, f"    å‰é¦ˆç½‘ç»œ")
                if hasattr(layer, 'cnl_1'):
                    count_parameters(layer.cnl_1, f"    ä¸Šä¸‹æ–‡å½’ä¸€åŒ–1")
                if hasattr(layer, 'cnl_2'):
                    count_parameters(layer.cnl_2, f"    ä¸Šä¸‹æ–‡å½’ä¸€åŒ–2")
                if hasattr(layer, 'cnl_3'):
                    count_parameters(layer.cnl_3, f"    ä¸Šä¸‹æ–‡å½’ä¸€åŒ–3")
                if hasattr(layer, 'fc_out') and layer.fc_out is not None:
                    count_parameters(layer.fc_out, f"    è¾“å‡ºå…¨è¿æ¥å±‚")
        else:
            params, trainable = count_parameters(component, f"  {name}")
    
    # è®¡ç®—å„ç»„ä»¶å æ¯”
    print("\n" + "=" * 60)
    print("å‚æ•°é‡å æ¯”åˆ†æ")
    print("=" * 60)
    
    components = [
        ("è§†è§‰æå–å™¨", visual_params),
        ("ç¼–ç å™¨", encoder_params),
        ("è§£ç å™¨", decoder_params),
        ("å¼•å¯¼è®°å¿†ç”Ÿæˆå™¨", gmg_params)
    ]
    
    for name, params in components:
        if params > 0:
            percentage = (params / total_params) * 100
            print(f"{name}: {params:,} å‚æ•° ({percentage:.1f}%)")
    
    # æ‰¾å‡ºæœ€å¤§ç»„ä»¶
    max_component = max([(name, params) for name, params in components if params > 0], 
                       key=lambda x: x[1])
    
    print(f"\nğŸ“Š æœ€å¤§å¯è®­ç»ƒå‚æ•°ç»„ä»¶: {max_component[0]}")
    print(f"ğŸ“ˆ å‚æ•°é‡: {max_component[1]:,} ({(max_component[1]/total_params)*100:.1f}%)")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Analyze EncoderDecoder parameters')
    parser.add_argument('--dataset_name', type=str, default='mimic_cxr', 
                       help='dataset name')
    parser.add_argument('--image_dir', type=str, default='data/mimic_cxr/images', 
                       help='directory of images')
    parser.add_argument('--ann_path', type=str, default='data/mimic_cxr/annotation.json', 
                       help='path to annotation file')
    parser.add_argument('--max_seq_length', type=int, default=100, 
                       help='maximum sequence length')
    parser.add_argument('--threshold', type=int, default=10, 
                       help='threshold for words')
    parser.add_argument('--num_workers', type=int, default=2, 
                       help='number of workers for dataloader')
    parser.add_argument('--batch_size', type=int, default=16, 
                       help='batch size')
    parser.add_argument('--d_vf', type=int, default=2048, 
                       help='dimension of visual features')
    parser.add_argument('--num_layers', type=int, default=1, 
                       help='number of layers')
    parser.add_argument('--rm_num_slots', type=int, default=3, 
                       help='read memory num slots')
    parser.add_argument('--rm_num_heads', type=int, default=8, 
                       help='read memory num heads')
    parser.add_argument('--rm_d_model', type=int, default=512, 
                       help='read memory d model')
    
    args = parser.parse_args()
    analyze_encoder_decoder_components(args)